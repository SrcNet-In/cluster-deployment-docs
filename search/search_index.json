{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"gateway-api/","title":"gateway-api","text":""},{"location":"gateway-api/#gateway-api","title":"Gateway API","text":""},{"location":"gateway-api/#premise","title":"Premise","text":"<p>To be generally available, web applications running on Kubernetes need to be exposed outside the cluster. This can be achieved in a number of ways.</p> <p>The easiest way to expose a service or API endpoint outside a Kubernetes cluster is to assign a service type of <code>NodePort</code>. The drawback of this approach is that the service runs on a non-standard port number. Also, each cluster has a finite number of ports assigned to its NodePort pool.</p> <p>These limitations can be overcome by using either the Ingress API or the Gateway API. While the Ingress API has been widely deployed in the Kubernetes world, it has its own drawbacks. Notably, a strong reliance on annotations in Ingress manifests makes the Ingress API inflexible and difficult to write generic templates for in Helm charts.</p> <p>The Gateway API overcomes these drawbacks by providing a generic, easy to templatise, and extensible way to define resources which provide an ingress of traffic to Kubernetes-hosted endpoints.</p> <p>In the following sections, we will compare the two approaches.</p>"},{"location":"gateway-api/#ingress-api","title":"Ingress API","text":"<p>HTTP and HTTPS network services may be exposed outside the Kubernetes cluster using Ingress resources, provided as part of the Ingress API. Traffic is routed using rules defined within the Ingress resource.</p> <p>Before Ingress resources can be defined, the cluster must have at least one Ingress Controller running. This Ingress Controller service is typically exposed using a <code>LoadBalancer</code> service type.</p> <p>The Ingress API is widely adopted by Kubernetes users and well-supported by vendors, with many implementations (Ingress controllers) available.</p>"},{"location":"gateway-api/#ingress-api-resource-organisation","title":"Ingress API resource organisation","text":""},{"location":"gateway-api/#limitations-of-the-ingress-api","title":"Limitations of the Ingress API","text":"<ul> <li>Limited features: The Ingress API only supports TLS termination and simple content-based request routing of HTTP traffic.</li> <li>Reliance on annotations for extensibility: The annotations approach leads to limited portability as every implementation has its own supported extensions.</li> <li>Insufficient permission model: The Ingress API is not well-suited for multi-team clusters with shared load-balancing infrastructure.</li> </ul>"},{"location":"gateway-api/#gateway-api_1","title":"Gateway API","text":"<p>The Gateway API is an official Kubernetes project being worked on by the Kubernetes Network SIG, representing the next generation of Ingress, Load Balancing, and Service Mesh APIs. It focuses on L4 and L7 routing within Kubernetes. It is designed to be generic, expressive, and role-oriented.</p>"},{"location":"gateway-api/#features-of-the-gateway-api","title":"Features of the Gateway API","text":"<p>The following design goals drive the concepts of Gateway API and demonstrate how Gateway aims to improve upon current standards like Ingress:</p> <ul> <li>Role-oriented: Gateway is composed of API resources which model organizational roles that use and configure Kubernetes service networking.</li> <li>Portable: Like Ingress, Gateway API is designed to be a portable specification supported by many implementations.</li> <li>Expressive: Gateway API supports features like header-based matching, traffic weighting, and others that were only possible in Ingress through custom annotations.</li> <li>Extensible: Allows for custom resources to be linked at various layers of the API for granular customization.</li> </ul>"},{"location":"gateway-api/#gateway-api-resource-model","title":"Gateway API Resource Model","text":"<ul> <li>GatewayClass: Cluster-scoped resource defining a set of Gateways that share a common configuration and behavior. Handled by a Gateway Controller.</li> <li>Gateway: Describes how traffic can be translated within a cluster. Acts as a gateway between external and internal traffic.</li> <li>Route Resources: Protocol-specific rules for mapping requests from a Gateway to Kubernetes Services:</li> <li><code>HTTPRoute</code>: Multiplexing HTTP or terminated HTTPS connections.</li> <li><code>GRPCRoute</code>: Routing gRPC traffic.</li> <li><code>TLSRoute</code> (experimental): Multiplexing TLS connections via SNI.</li> <li><code>TCPRoute</code> and <code>UDPRoute</code> (experimental): Mapping TCP/UDP ports to backends. May terminate TLS where appropriate.</li> </ul>"},{"location":"gateway-api/#gateway-api-resource-organisation","title":"Gateway API resource organisation","text":"<p>Reference: </p>"},{"location":"gateway-api/#examples","title":"Examples","text":"<p>The following example snippets illustrate the differences between Ingress API and Gateway API YAML manifests. Both create traffic routes to the same Service resource.</p> <p>Note: Installation of an Ingress Controller and Gateway Controller are outside the scope of this document.</p>"},{"location":"gateway-api/#ingress-api_1","title":"Ingress API","text":"<pre><code>apiVersion: networking.k8s.io/v1\nkind: IngressClass\nmetadata:\n  labels:\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/version: 1.12.0-beta.0\n  name: nginx\nspec:\n  controller: k8s.io/ingress-nginx\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: science-portal-ingress\n  namespace: skaha-system\n  annotations:\n    spec.ingressClassName: nginx\nspec:\n  ingressClassName: nginx\n  rules:\n    - host: canfar.e4r.internal\n      http:\n        paths:\n          - path: /science-portal\n            pathType: Prefix\n            backend:\n              service:\n                name: science-portal-tomcat-svc\n                port:\n                  number: 8080\n</code></pre>"},{"location":"gateway-api/#gateway-api_2","title":"Gateway API","text":"<pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: GatewayClass\nmetadata:\n  name: envoy-gateway-class\nspec:\n  controllerName: gateway.envoyproxy.io/gatewayclass-controller\n---\napiVersion: gateway.networking.k8s.io/v1\nkind: Gateway\nmetadata:\n  name: envoy-canfar-gateway\nspec:\n  gatewayClassName: envoy-gateway-class\n  listeners:\n    - name: canfar-gateway-http-envoy\n      protocol: HTTP\n      port: 80\n      allowedRoutes:\n        namespaces:\n          from: All\n    - name: canfar-gateway-https-envoy\n      protocol: HTTPS\n      port: 443\n      allowedRoutes:\n        namespaces:\n          from: All\n      hostname: \"canfar.e4r.internal\"\n      tls:\n        certificateRefs:\n          - kind: Secret\n            group: \"\"\n            name: canfar-gateway-tls\n---\napiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\n  name: envoy-canfar-gateway-httproutes\n  namespace: skaha-system\nspec:\n  hostnames:\n    - \"canfar.e4r.internal\"\n  parentRefs:\n    - name: envoy-canfar-gateway\n      namespace: default\n  rules:\n    - backendRefs:\n        - name: science-portal-tomcat-svc\n          port: 8080\n      matches:\n        - path:\n            type: PathPrefix\n            value: /science-portal\n</code></pre>"},{"location":"gateway-api/#example-for-canfar-deployment-using-gateway-api","title":"Example for Canfar deployment using Gateway API","text":"<p>The following snippet is an example to deploy Canfar using Gateway API.</p>"},{"location":"gateway-api/#step-1-create-a-gatewayclass","title":"Step 1: Create a GatewayClass","text":"<pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: GatewayClass\nmetadata:\n  name: envoy-gateway-class\nspec:\n  controllerName: gateway.envoyproxy.io/gatewayclass-controller\n</code></pre> <p>The <code>controllerName</code> refers to the controller that will manage Gateways of this class.</p>"},{"location":"gateway-api/#step-2-create-a-gateway-for-canfar","title":"Step 2: Create a Gateway for Canfar","text":"<pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: Gateway\nmetadata:\n  name: envoy-canfar-gateway\nspec:\n  gatewayClassName: envoy-gateway-class\n  listeners:\n    - name: canfar-gateway-http-envoy\n      protocol: HTTP\n      port: 80\n      allowedRoutes:\n        namespaces:\n          from: All\n    - name: canfar-gateway-https-envoy\n      protocol: HTTPS\n      port: 443\n      allowedRoutes:\n        namespaces:\n          from: All\n      hostname: \"canfar.e4r.internal\"\n      tls:\n        certificateRefs:\n          - kind: Secret\n            group: \"\"\n            name: canfar-gateway-tls\n</code></pre>"},{"location":"gateway-api/#step-3-create-an-httproute-for-services-in-canfar","title":"Step 3: Create an HTTPRoute for services in Canfar","text":"<pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\n  name: envoy-canfar-gateway-httproutes\n  namespace: skaha-system\nspec:\n  hostnames:\n    - \"canfar.e4r.internal\"\n  parentRefs:\n    - name: envoy-canfar-gateway\n      namespace: default\n  rules:\n    - backendRefs:\n        - name: gms\n          port: 8080\n      matches:\n        - path:\n            type: PathPrefix\n            value: /gms\n    - backendRefs:\n        - name: reg\n          port: 8080\n      matches:\n        - path:\n            type: PathPrefix\n            value: /reg\n    - backendRefs:\n        - name: science-portal-tomcat-svc\n          port: 8080\n      matches:\n        - path:\n            type: PathPrefix\n            value: /science-portal\n    - backendRefs:\n        - name: skaha-tomcat-svc\n          port: 8080\n      matches:\n        - path:\n            type: PathPrefix\n            value: /skaha\n</code></pre>"},{"location":"gateway-api/#step-4-create-a-gateway-for-harbor-and-attach-the-gatewayclass-created-in-step-1","title":"STEP 4: Create a Gateway for Harbor and attach the GatewayClass created in STEP 1.","text":"<pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: Gateway\nmetadata:\n  name: envoy-harbor-gateway\nspec:\n  gatewayClassName: envoy-gateway-class\n  # The same GatewayClass is used for Harbor.\n  listeners:\n    - name: harbor-gateway-http-envoy\n      protocol: HTTP\n      port: 80\n      allowedRoutes:\n        namespaces:\n          from: All\n      # This listener will accept HTTP traffic on port 80 from any namespace.\n\n    - name: harbor-gateway-https-envoy\n      protocol: HTTPS\n      port: 443\n      allowedRoutes:\n        namespaces:\n          from: All\n      # This listener will accept HTTPS traffic on port 443 from any namespace.\n      hostname: \"harbor.e4r.internal\"\n      # Define a hostname for the Harbor gateway.\n      tls:\n        certificateRefs:\n          - kind: Secret\n            group: \"\"\n            name: harbor-gateway-tls\n          # The TLS configuration references a Secret containing the TLS certificate.\n</code></pre>"},{"location":"gateway-api/#step-5-create-an-httproute-for-the-services-in-harbor-and-attach-it-to-the-gateway-created-in-step-4","title":"STEP 5: Create an HTTPRoute for the services in Harbor and attach it to the Gateway created in STEP 4.","text":"<pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\n  name: harbor-httproutes\n  namespace: harbor\nspec:\n  hostnames:\n    - \"harbor.e4r.internal\"\n    # HTTPRoute applies to traffic matching this hostname.\n  parentRefs:\n    - name: envoy-harbor-gateway\n      namespace: default\n    # This HTTPRoute is attached to the 'envoy-harbor-gateway' Gateway defined earlier.\n  rules:\n    # Define routing rules for various paths within Harbor\n    - backendRefs:\n        - name: harbor-portal\n          port: 80\n      matches:\n        - path:\n            type: PathPrefix\n            value: /\n    - backendRefs:\n        - name: harbor-core\n          port: 80\n      matches:\n        - path:\n            type: PathPrefix\n            value: /c\n    - backendRefs:\n        - name: harbor-core\n          port: 80\n      matches:\n        - path:\n            type: PathPrefix\n            value: /chartrepo\n    - backendRefs:\n        - name: harbor-core\n          port: 80\n      matches:\n        - path:\n            type: PathPrefix\n            value: /v2\n    - backendRefs:\n        - name: harbor-core\n          port: 80\n      matches:\n        - path:\n            type: PathPrefix\n            value: /service\n    - backendRefs:\n        - name: harbor-core\n          port: 80\n      matches:\n        - path:\n            type: PathPrefix\n            value: /api\n</code></pre>"},{"location":"gateway-api/#reference-resources","title":"Reference Resources","text":"<ul> <li>https://gateway-api.sigs.k8s.io/guides/migrating-from-ingress/#reasons-to-switch-to-gateway-api</li> <li>https://cloud.google.com/kubernetes-engine/docs/concepts/gateway-api</li> <li>https://gateway-api.sigs.k8s.io/guides/simple-gateway/</li> </ul>"},{"location":"kepler/","title":"kepler","text":""},{"location":"kepler/#kepler-kubernetes-based-efficient-power-level-exporter","title":"Kepler: Kubernetes-based Efficient Power Level Exporter","text":""},{"location":"kepler/#what-is-kepler","title":"What is Kepler?","text":"<p>Kepler stands for Kubernetes-based Efficient Power Level Exporter. It is a Prometheus exporter, an open-source project designed to estimate and monitor the energy consumption and CO\u2082 emissions of workloads running within Kubernetes clusters. It uses eBPF to monitor CPU performance counters and Linux kernel tracepoints. Data and stats from sysfs can be fed into ML models to estimate energy consumption by Pods.</p>"},{"location":"kepler/#how-kepler-works","title":"How Kepler Works?","text":"<p>Kepler uses different methods to collect power data and share it through Prometheus.</p>"},{"location":"kepler/#requirements","title":"Requirements","text":"Component Requirement Linux Kernel <code>&gt;= 5.12</code> (eBPF features used by Kepler require a modern kernel version) eBPF Programs - Must run in privileged containers - Requires CAP_SYS_ADMIN, CAP_BPF capabilities Metrics Exporter Needs a <code>ClusterRole</code> and <code>RoleBinding</code> with <code>get</code>, <code>list</code>, and <code>watch</code> on pods, nodes/metrics, nodes/proxy, nodes/stats Prometheus Prometheus must be configured to scrape Kepler metrics via <code>ServiceMonitor</code> Grafana Prometheus configured as data source Helm Helm v3 for deploying Kepler and Prometheus stack"},{"location":"kepler/#data-collection-methods","title":"Data Collection Methods","text":"Data Source Description Permissions Required eBPF (Extended Berkeley Packet Filter) Kernel-level monitoring of process counters and system stats Privileged Container Intel RAPL CPU/DRAM power consumption Read access to RAPL MSRs NVIDIA NVML GPU power usage data Requires NVIDIA drivers installed SPECPower Estimates Power usage predicted using benchmark correlations No special permissions IPMI Platform-wide power metrics via firmware interfaces May require root / elevated access"},{"location":"kepler/#how-ml-models-help","title":"How ML Models Help","text":"<p>In environments where real-time hardware metrics (e.g., RAPL, NVML) are unavailable, Kepler uses machine learning models (typically regression models) trained on benchmarked datasets like SPECPower.</p> <p>These models estimate power consumption based on inputs like:</p> <ul> <li>CPU Cycles</li> <li>Memory Usage</li> <li>I/O Metrics</li> <li>Network Throughput</li> </ul> <p>This ensures broader compatibility, allowing Kepler to work across diverse infrastructure types\u2014even virtual machines or systems without energy interfaces</p> <p>Note: Kepler currently relies heavily on eBPF and requires low-level system access (i.e., privileged container mode) to function correctly.</p>"},{"location":"kepler/#understanding-ebpf","title":"Understanding eBPF","text":"<p>eBPF (Extended Berkeley Packet Filter) is a technology that allows programs to run in the kernel space of an operating system without modifying the kernel itself. It enables safe and efficient monitoring and manipulation of kernel behavior. eBPF programs are attached to various hooks in the kernel, such as system calls, tracepoints, and network events. When these events occur, the eBPF programs are triggered to execute.</p> <p>In the context of Kepler, eBPF is used to collect fine-grained power consumption metrics by accessing hardware performance counters and other system statistics. This enables accurate estimation of energy usage at the container level.</p>"},{"location":"kepler/#kepler-architecture","title":"Kepler Architecture","text":"<p>Source: https://sustainable-computing.io/design/architecture/</p>"},{"location":"kepler/#installation-of-kepler","title":"Installation of Kepler","text":"<p>Kepler utilizes eBPF to gather detailed power consumption metrics. This requires elevated privileges to load and execute eBPF programs within the kernel.</p> <p>Installation Steps:</p> <ol> <li>Install Prometheus Stack (if Prometheus isn't already installed):</li> </ol> <pre><code>helm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm repo update\nhelm install prometheus prometheus-community/kube-prometheus-stack --namespace monitoring --create-namespace --wait\n</code></pre> <ol> <li>Install Kepler:</li> </ol> <pre><code>helm repo add kepler https://sustainable-computing-io.github.io/kepler-helm-chart\nhelm repo update\nhelm install kepler kepler/kepler \\\n  --namespace monitoring \\\n  --set serviceMonitor.enabled=true \\\n  --set serviceMonitor.labels.release=prometheus \\\n  --set securityContext.privileged=true\n</code></pre> <ol> <li>Verify Installation:</li> </ol> <pre><code>kubectl get pods -n monitoring\n</code></pre> <ol> <li> <p>Integrate with Grafana:</p> </li> <li> <p>Import Kepler dashboard JSON into Grafana.</p> </li> <li>Refresh the browser window to load the new dashboard.</li> </ol>"},{"location":"kepler/#references","title":"References","text":"<ul> <li>Kepler GitHub Repository</li> <li>Kepler Installation Guide</li> <li>Kepler Deep Dive</li> <li>Kepler on CNCF</li> <li>Kepler Architecture Overview</li> <li>eBPF Wikipedia</li> <li>Kepler pod fail when no access to eBPF</li> </ul>"},{"location":"logging/","title":"logging","text":""},{"location":"logging/#observability-setup-with-loki-alloy-in-kubernetes","title":"Observability Setup with Loki &amp; Alloy in Kubernetes","text":"<p>This guide walks you through deploying Grafana Loki for log aggregation and Grafana Alloy for log collection in Kubernetes. It is assumed that Grafana is already installed and accessible.</p>"},{"location":"logging/#tools-used","title":"Tools Used","text":"<ul> <li>Grafana: For visualizing logs using Loki as a data source.</li> <li>Grafana Loki: A log aggregation system tailored for Kubernetes.</li> <li>Grafana Alloy: A lightweight, flexible collector for logs, metrics, and traces.</li> </ul>"},{"location":"logging/#loki-installation-monolithic-mode","title":"Loki Installation (Monolithic Mode)","text":"<p>We will install Loki in monolithic mode for this example, which is simpler and ideal for small to medium clusters.</p>"},{"location":"logging/#1-create-namespace","title":"1. Create Namespace","text":"<pre><code>kubectl create namespace monitoring\n</code></pre>"},{"location":"logging/#2-add-grafana-helm-repository","title":"2. Add Grafana Helm Repository","text":"<pre><code>helm repo add grafana https://grafana.github.io/helm-charts\nhelm repo update\n</code></pre>"},{"location":"logging/#3-get-loki-default-values","title":"3. Get Loki Default Values","text":"<pre><code>helm show values grafana/loki &gt; values.yaml\n</code></pre>"},{"location":"logging/#4-edit-valuesyaml-for-monolithic-deployment","title":"4. Edit <code>values.yaml</code> for Monolithic Deployment","text":"<pre><code>loki:\n  auth_enabled: false\n  commonConfig:\n    replication_factor: 1\n  schemaConfig:\n    configs:\n      - from: \"2025-04-01\"\n        store: tsdb\n        object_store: s3\n        schema: v13\n        index:\n          prefix: loki_index_\n          period: 24h\n  pattern_ingester:\n    enabled: true\n  limits_config:\n    allow_structured_metadata: true\n    volume_enabled: true\n  ruler:\n    enable_api: true\n\nminio:\n  enabled: true\n\ndeploymentMode: SingleBinary\n\nsingleBinary:\n  replicas: 1\n\nbackend:\n  replicas: 0\nread:\n  replicas: 0\nwrite:\n  replicas: 0\ningester:\n  replicas: 0\nquerier:\n  replicas: 0\nqueryFrontend:\n  replicas: 0\nqueryScheduler:\n  replicas: 0\ndistributor:\n  replicas: 0\ncompactor:\n  replicas: 0\nindexGateway:\n  replicas: 0\nbloomCompactor:\n  replicas: 0\nbloomGateway:\n  replicas: 0\n</code></pre>"},{"location":"logging/#5-install-loki","title":"5. Install Loki","text":"<pre><code>helm install loki grafana/loki -n monitoring -f values.yaml\n</code></pre>"},{"location":"logging/#loki-deployment-modes","title":"Loki Deployment Modes","text":"<p>Loki supports three modes:</p> <ol> <li>Monolithic (SingleBinary): All components run in a single process. Best for dev/staging/small prod.</li> <li>Simple Scalable: Components like ingester, querier, etc., are split out but share a single process per replica.</li> <li>Microservices Mode: Full separation of components (e.g., distributor, ingester, querier). Ideal for large-scale clusters.</li> </ol> Feature Monolithic (SingleBinary) Simple Scalable Microservices Mode Description All components run as a single binary process. Components like ingester, querier, etc. are separated into individual components but still share a single process per replica. Full separation of components (e.g., distributor, ingester, querier). Each component runs as a separate service. Ideal Use Case Small to medium clusters, development, or staging environments. Medium-scale clusters with moderate scalability needs. Large-scale, high-traffic environments with high availability needs. Scalability Limited scalability (all components are together in a single process). Can scale individual components (e.g., querier, ingester) independently. Highly scalable, full independent scaling of each component. Operational Complexity Low, easy to deploy and manage. Moderate, needs separate management for individual components. High, each component must be managed and scaled independently. Fault Tolerance Single point of failure (if the binary fails, everything fails). Better fault tolerance than monolithic (individual components can fail without taking down the entire system). High fault tolerance, as failure in one component does not affect others. Resource Efficiency More resource-efficient as it uses a single process. More resource-intensive as individual components run in separate processes. Most resource-intensive, as each component has its own resources and scaling. Ideal For Small-scale setups, testing, or development environments. Medium-scale setups requiring some scalability but avoiding full microservices complexity. Large-scale production environments requiring high resilience, scalability, and fault tolerance. <p>For more details on deployment modes, see the Loki deployment modes documentation.</p>"},{"location":"logging/#configure-alloy-for-log-collection","title":"Configure Alloy for Log Collection","text":""},{"location":"logging/#1-download-default-alloy-values","title":"1. Download Default Alloy Values","text":"<pre><code>helm show values grafana/alloy &gt; values.yaml\n</code></pre>"},{"location":"logging/#2-edit-valuesyaml-for-alloy-configuration","title":"2. Edit <code>values.yaml</code> for Alloy Configuration","text":"<pre><code>alloy:\n  configMap:\n    content: |-\n      loki.write \"default\" {\n        endpoint {\n          url = \"http://loki.monitoring:3100/loki/api/v1/push\"\n        }\n      }\n\n      discovery.kubernetes \"pod\" {\n        role = \"pod\"\n      }\n\n      discovery.relabel \"pod_logs\" {\n        targets = discovery.kubernetes.pod.targets\n\n        rule {\n          source_labels = [\"__meta_kubernetes_namespace\"]\n          action        = \"replace\"\n          target_label  = \"namespace\"\n        }\n\n        rule {\n          source_labels = [\"__meta_kubernetes_pod_name\"]\n          action        = \"replace\"\n          target_label  = \"pod\"\n        }\n\n        rule {\n          source_labels = [\"__meta_kubernetes_pod_container_name\"]\n          action        = \"replace\"\n          target_label  = \"container\"\n        }\n\n        rule {\n          source_labels = [\"__meta_kubernetes_namespace\", \"__meta_kubernetes_pod_container_name\"]\n          action        = \"replace\"\n          target_label  = \"job\"\n          separator     = \"/\"\n          replacement   = \"$1/$2\"\n        }\n\n        rule {\n          source_labels = [\"__meta_kubernetes_pod_uid\", \"__meta_kubernetes_pod_container_name\"]\n          action        = \"replace\"\n          target_label  = \"__path__\"\n          separator     = \"/\"\n          replacement   = \"/var/log/pods/*$1/*.log\"\n        }\n      }\n\n      loki.source.kubernetes \"pod_logs\" {\n        targets    = discovery.relabel.pod_logs.output\n        forward_to = [loki.process.pod_logs.receiver]\n      }\n\n      loki.process \"pod_logs\" {\n        stage.static_labels {\n          values = {\n            cluster = \"kubernetes\"\n          }\n        }\n        forward_to = [loki.write.default.receiver]\n      }\n</code></pre>"},{"location":"logging/#install-alloy-with-helm","title":"Install Alloy with Helm","text":"<pre><code>helm install alloy grafana/alloy -n monitoring -f values.yaml\n</code></pre>"},{"location":"logging/#add-loki-as-a-data-source-in-grafana","title":"Add Loki as a Data Source in Grafana","text":"<ol> <li>Open Grafana \u2192 Connections \u2192 Data Sources \u2192 Add Data Source.</li> <li>Choose Loki.</li> <li>URL: <code>http://loki.monitoring.svc.cluster.local:3100</code></li> <li>Click Save &amp; Test.</li> </ol> <p>You can now explore logs under Explore \u2192 Logs.</p>"},{"location":"logging/#storage-retention-archive","title":"Storage, Retention &amp; Archive","text":"<p>Loki can use object storage like:</p> <ul> <li>Amazon S3</li> <li>Google Cloud Storage (GCS)</li> <li>Azure Blob</li> </ul>"},{"location":"logging/#example-store-logs-in-amazon-s3-with-a-60-day-retention-period","title":"Example: Store Logs in Amazon S3 with a 60-Day Retention Period","text":"<pre><code>loki:\n  auth_enabled: false\n  commonConfig:\n    replication_factor: 1\n  schemaConfig:\n    configs:\n      - from: \"2025-04-01\"\n        store: tsdb\n        object_store: s3\n        schema: v13\n        index:\n          prefix: loki_index_\n          period: 24h\n  storage_config:\n    aws:\n      s3: s3://access_key:secret_key@region/bucket_name\n      s3forcepathstyle: true\n  limits_config:\n    allow_structured_metadata: true\n    volume_enabled: true\n    retention_period: 1440h # 60 days\n  compactor:\n    enabled: true\n    retention_enabled: true\n    compaction_interval: 10m\n    shared_store: s3\n  ruler:\n    enable_api: true\ndeploymentMode: SingleBinary\nsingleBinary:\n  replicas: 1\nbackend:\n  replicas: 0\nread:\n  replicas: 0\nwrite:\n  replicas: 0\ningester:\n  replicas: 0\nquerier:\n  replicas: 0\nqueryFrontend:\n  replicas: 0\nqueryScheduler:\n  replicas: 0\ndistributor:\n  replicas: 0\ncompactor:\n  replicas: 0\nindexGateway:\n  replicas: 0\nbloomCompactor:\n  replicas: 0\nbloomGateway:\n  replicas: 0\n</code></pre>"},{"location":"logging/#sample-amazon-s3-lifecycle-policy","title":"Sample Amazon S3 lifecycle policy","text":"<pre><code>{\n  \"Rules\": [\n    {\n      \"ID\": \"ArchiveToGlacierAfter60Days\",\n      \"Filter\": { \"Prefix\": \"\" },\n      \"Status\": \"Enabled\",\n      \"Transitions\": [\n        {\n          \"Days\": 60,\n          \"StorageClass\": \"GLACIER\"\n        }\n      ],\n      \"Expiration\": {\n        \"Days\": 120     # optional\n      }\n    }\n  ]\n}\n</code></pre>"},{"location":"logging/#storage-retrieval-sequence","title":"Storage &amp; Retrieval Sequence","text":"<ul> <li> <p>Day 0 \u2013 Ingestion &amp; Initial S3 Upload   Loki writes each new log as a TSDB chunk + index file and immediately pushes them into your S3 bucket.</p> </li> <li> <p>Day 60 \u2013 S3 Lifecycle Transition to Glacier   Your S3 lifecycle rule moves every object older than 60 days from \u201chot\u201d S3 into Glacier (or Glacier Deep Archive) for cost-effective archiving.</p> </li> <li> <p>Day 61 \u2013 Loki Retention Enforcement   The Loki Compactor scans for data older than 60 days and drops its index pointers so those logs no longer appear in Grafana queries\u2014but does not delete the S3/Glacier objects.</p> </li> <li> <p>Optional: Day 120 \u2013 S3 Permanent Deletion (Skip this if you want to retain data in Glacier forever.)   A second S3 rule can expire (delete) Glacier objects after e.g. 120 days\u2014otherwise they remain indefinitely.</p> </li> <li> <p>Restoring &amp; Re-ingesting Old Logs (e.g., 3 Years Later)</p> </li> <li>Glacier Restore \u2192 S3</li> <li>Download Restored Files Locally <pre><code>aws s3 sync s3://my-loki-bucket/chunks/ ./local-chunks/ --recursive --include=\"*.chunk\" --force-glacier-transfer\naws s3 sync s3://my-loki-bucket/index/ ./local-index/ --recursive --include=\"*.index\" --force-glacier-transfer\n</code></pre></li> <li>Decode TSDB Chunks to Plain Logs      Use Prometheus\u2019s built-in tool:      <pre><code>promtool tsdb dump \\\n  --block-dir=./local-chunks/&lt;block-id&gt; \\\n  --output=./extracted-logs/&lt;block-id&gt;.txt\n</code></pre></li> <li>Prepare Alloy Replay Config      Mount <code>./extracted-logs/</code> into your Alloy pod, then modify grafana config:      <pre><code>local.file_match \"old_logs\" {\n  path_targets = [{ __path__ = \"/mnt/extracted-logs/**/*.txt\" }]\n}\nloki.source.file \"replay_old\" {\n  targets    = local.file_match.old_logs.targets\n  forward_to = [loki.write.out]\n}\nloki.write \"out\" {\n  endpoint { url = \"http://loki:3100/loki/api/v1/push\" }\n  timeout  = \"30s\"\n}\n</code></pre></li> <li>Deploy Alloy &amp; Replay</li> <li>Query in Grafana</li> </ul> <p>Compression: Loki automatically compresses logs using Snappy before storing them in object storage. For more details Loki Snappy Compression Algorithm.</p>"},{"location":"logging/#references","title":"References","text":"<ul> <li>Loki Docs</li> <li>Alloy Docs</li> <li>Loki Deployment Modes</li> <li>Loki Deployment Guides</li> <li>Loki Architecture</li> <li>Loki Storage</li> </ul>"},{"location":"observability/","title":"observability","text":""},{"location":"observability/#kubernetes-observability-tooling","title":"Kubernetes Observability Tooling","text":""},{"location":"observability/#tech-stack","title":"Tech Stack","text":"<p>In our POC implementation for Kubernetes observability tooling for SRCNET Nodes, we have opted for the PLG stack\u2014comprising of Prometheus, Loki, and Grafana\u2014over the ELK stack (ElasticSearch, Logstash, and Kibana).</p> <p>This stack offers the following features:</p> <ol> <li> <p>Kubernetes Native: The PLG stack is inherently designed for Kubernetes environments, offering seamless integration with Kubernetes-based workloads and configurations. This makes it a more natural choice for our observability needs within a Kubernetes cluster.</p> </li> <li> <p>Simple and Efficient: The PLG stack is known for its simplicity in setup and operation, while also being highly efficient in handling large volumes of data. It provides a streamlined approach to monitoring and logging, reducing complexity compared to the more generalized ELK stack.</p> </li> <li> <p>Scalable: PLG is designed with scalability in mind, which is crucial for managing large-scale Kubernetes deployments. As our observability needs grow, PLG's architecture can easily scale to accommodate increased traffic and data volume.</p> </li> <li> <p>Performent and Easy to Maintenance: In addition to its ease of use, the PLG stack offers high performance and low resource consumption, making it a better fit.</p> </li> </ol>"},{"location":"observability/#list-of-tools","title":"List of Tools","text":"<ol> <li>Grafana</li> <li>Prometheus</li> <li>Loki</li> <li>Alloy</li> <li>Tempo</li> <li>OpenTelemetry Collector</li> </ol>"},{"location":"observability/#grafana","title":"Grafana","text":"<p>Grafana is an open-source tool used to create visual dashboards for monitoring and analyzing data. It connects to various data sources like Prometheus, Loki and helps you to visualize metrics in real-time. Grafana makes it easy to track performance and identify issues with interactive charts and graphs.</p>"},{"location":"observability/#prometheus","title":"Prometheus","text":"<p>Prometheus is a monitoring and alerting toolkit designed for Kubernetes and cloud-native environments. It collects and stores metrics (data on system performance) over time. Prometheus automatically scrapes data from your services and stores it in a time-series database.</p>"},{"location":"observability/#grafana-loki","title":"Grafana Loki","text":"<p>Loki is a log aggregation system designed for cloud-native environments, particularly Kubernetes. It efficiently collects, stores, and indexes logs from applications and services. Unlike traditional logging systems, Loki indexes logs by labels (such as application name, pod, or container), rather than the full content of each log line. This approach significantly reduces storage requirements and makes querying faster and more cost-effective.</p>"},{"location":"observability/#grafana-alloy","title":"Grafana Alloy","text":"<p>Grafana Alloy is an open-source distribution of the OpenTelemetry Collector, designed to streamline the collection, processing, and export of telemetry data\u2014including metrics, logs, traces, and profiles\u2014in environments like Kubernetes. It offers native support for both OpenTelemetry and Prometheus telemetry formats.</p>"},{"location":"observability/#grafana-tempo","title":"Grafana Tempo","text":"<p>Grafana Tempo is an open-source, high-scale distributed tracing backend. It enables efficient collection, storage, and analysis of trace data, providing valuable insights into the performance and reliability of distributed systems and it integrates nicely with grafana loki for logs and prometheus for metrics</p>"},{"location":"observability/#opentelemetry-collector","title":"OpenTelemetry Collector","text":"<p>The OpenTelemetry Collector is a vendor-agnostic tool designed to receive, process, and export telemetry data such as metrics, logs, and traces. When deployed within Kubernetes clusters, it enhances observability by collecting and managing telemetry data from various sources.</p>"},{"location":"observability/#system-requirements","title":"System Requirements","text":"<p>To set up the observability stack for SRCNodes, which includes Grafana, Prometheus, Loki, Tempo, Alloy, and OpenTelemetry Collector, ensure that the following hardware and software prerequisites are met.</p>"},{"location":"observability/#hardware-requirements","title":"Hardware Requirements","text":"<p>CPU: Minimum of 2 cores per node.\u200b Memory: At least 2 GB of RAM per node Storage: 20 GB of free disk space per node.</p> <p>* Prometheus and Loki will require disk space for storing time-series metrics and logs when they are stored locally instead of cloud services like S3 or GCS. * Tempo may also require significant storage depending on the retention period for traces.</p>"},{"location":"observability/#software-requirements","title":"Software Requirements","text":"<p>Kubernetes: 1.19 (or later)</p> <p>Helm: v3.x (or later)</p>"},{"location":"observability/#architecture-overview","title":"Architecture Overview","text":"<p>In our POC environment, we created two namespaces:</p> <ol> <li> <p>Monitoring Namespace: This namespace is dedicated to deploying the monitoring-related stack, including components such as Prometheus, Loki, Tempo, Alloy and Grafana.</p> </li> <li> <p>Services Namespace: This namespace is where our test service is deployed. It serves as the target for monitoring and observability tasks, allowing us to collect and visualize metrics, logs, and traces.</p> </li> </ol> <p>The observability stack fullfills three requirments:</p> <ul> <li>Metrics Collection: Metrics are fetched from Prometheus and visualized in Grafana.</li> <li>Log Aggregation: Logs are gathered by Loki and displayed in Grafana.</li> <li>Distributed Tracing: Traces are collected from Tempo and rendered in Grafana.</li> </ul>"},{"location":"observability/#metrics-collection","title":"Metrics Collection","text":"<p>To capture metrics from services into Prometheus, there are several approaches. One method used in this architecture is Prometheus client integration. The process involves the following steps:</p> <ol> <li>Install Prometheus Client: The Prometheus client is installed within the service.</li> <li>Configure Metrics and Endpoint: The client is configured to collect the desired metrics and expose them via a dedicated endpoint.</li> <li>ServiceMonitor Creation: A ServiceMonitor resource is created to enable Prometheus to scrape the metrics at regular intervals.</li> <li>Data Storage: The collected metrics are stored in a Persistent Volume Claim (PVC) for durability.</li> <li>Grafana Integration: Prometheus is added as a data source to Grafana, which visualizes and presents the metrics.</li> </ol> <p>With this setup, Prometheus continuously scrapes the metrics from the service, and Grafana allows for real-time visualization of the data.</p>"},{"location":"observability/#logs-aggregation","title":"Logs Aggregation","text":"<p>To capture logs from services running within our Kubernetes cluster, we utilize Grafana Alloy in conjunction with Loki and Grafana for efficient log aggregation and visualization. The log collection and visualization process is as follows:</p> <ol> <li>Deploy Grafana Alloy: Grafana Alloy is installed within the Kubernetes cluster to collect logs from the pods. It offers flexibility in log collection methods, including using the Kubernetes API to tail logs without requiring a DaemonSet</li> <li>Configure Log Collection: Alloy is configured to collect logs from specific pods or namespaces. This configuration involves setting up the appropriate components within Alloy to discover and tail logs from the desired sources.</li> <li>Forward Logs to Loki: The collected logs are forwarded to Loki, a log aggregation system designed for scalability and efficiency. Alloy's loki.write component handles the delivery of logs to Loki endpoints.</li> <li>Store Logs in Loki: Loki stores the logs in an object storage system or persistent volume, ensuring durability and facilitating efficient querying.</li> <li>Visualize Logs in Grafana: Grafana is configured to use Loki as a data source. This setup allows for the creation of dynamic dashboards and the execution of queries to analyze and visualize log data in real-time.</li> </ol>"},{"location":"observability/#distributed-tracing","title":"Distributed Tracing","text":"<p>To implement distributed tracing within our Kubernetes environment, we utilize OpenTelemetry for instrumentation, the OpenTelemetry Collector for collecting and exporting trace data, and Grafana Tempo for storing and visualizing traces. The process is structured as follows:</p> <ol> <li> <p>Instrument Services with OpenTelemetry SDKs: Each service in the Services Namespace is instrumented using OpenTelemetry SDKs. This involves integrating the appropriate OpenTelemetry libraries into the application code to capture trace data, such as request paths, durations, and dependencies.</p> </li> <li> <p>Enable Auto-Instrumentation (Optional): For languages and frameworks that support it, auto-instrumentation is enabled to automatically capture trace data without manual code modifications. This approach simplifies the instrumentation process and ensures consistent trace data collection across services.</p> </li> <li>Deploy the OpenTelemetry Collector: The OpenTelemetry Collector is deployed within the Kubernetes cluster to receive, process, and export trace data. It acts as a telemetry gateway, collecting traces from instrumented services and forwarding them to the tracing backend.</li> <li>Configure the Collector to Export Traces to Grafana Tempo: The Collector is configured with exporters that send trace data to Grafana Tempo, our chosen tracing backend. This setup involves specifying the Tempo endpoint and ensuring that traces are transmitted securely and efficiently.</li> <li>Visualize Traces in Grafana: Grafana is configured to connect to Grafana Tempo as a data source. This integration allows for the creation of dashboards that visualize trace data, enabling the monitoring of request flows, identification of performance bottlenecks, and analysis of service dependencies.</li> </ol>"},{"location":"observability/#setting-up-the-plg-stack","title":"Setting up the PLG Stack","text":""},{"location":"observability/#kube-prometheus-stack","title":"kube-prometheus-stack:","text":"<p>To install the <code>kube-prometheus-stack</code> with customized configurations, follow these steps:</p> <p>1. Create a Monitoring Namespace:</p> <pre><code>kubectl create namespace monitoring\n</code></pre> <p>2. Add the Prometheus Community Helm Repository:</p> <pre><code>helm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm repo update\n</code></pre> <p>3. Fetch Default Values:</p> <p>Retrieve the default values of the <code>kube-prometheus-stack</code> Helm chart to customize them as needed:</p> <pre><code>helm show values prometheus-community/kube-prometheus-stack &gt; values.yaml\n</code></pre> <p>This command saves the default values to a <code>values.yaml</code> file, which you can edit to tailor the installation to your requirements.</p> <p>4. Customize values.yaml</p> <pre><code>prometheus:\n  prometheusSpec:\n    storageSpec:\n      volumeClaimTemplate:\n        metadata:\n          name: prometheus-pvc\n        spec:\n          accessModes:\n            - ReadWriteOnce\n          resources:\n            requests:\n              storage: 10Gi\n          storageClassName: nfs-default\n    retention: \"720h\"  # 30 days retention (720 hours)\n    namespaceSelector: {}\n\ngrafana:\n  persistence:\n    enabled: true\n    size: 5Gi\n    storageClassName: nfs-default\n\nalertmanager:\n  alertmanagerSpec:\n    storage:\n      volumeClaimTemplate:\n        metadata:\n          name: alertmanager-pvc\n        spec:\n          accessModes:\n            - ReadWriteOnce\n          resources:\n            requests:\n              storage: 5Gi\n          storageClassName: nfs-default\n</code></pre> <p>5. Install the <code>kube-prometheus-stack</code> Helm Chart:</p> <p>With your customized <code>values.yaml</code>, install the Helm chart in the <code>monitoring</code> namespace:</p> <pre><code>helm install kube-prometheus-stack prometheus-community/kube-prometheus-stack -n monitoring -f values.yaml\n</code></pre> <p>6. Set Up Gateway API Resources for Grafana Access:</p> <p>To access the Grafana dashboard via the Gateway API, define <code>Gateway</code> and <code>HTTPRoute</code> resources.</p> <pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: Gateway\nmetadata:\n  name: envoy-grafana-gateway\nspec:\n  gatewayClassName: envoy-gateway-class\n  listeners:\n  - name: grafana-gateway-http-envoy\n    protocol: HTTP\n    port: 80\n    allowedRoutes:\n        namespaces:\n          from: All\n  - name: grafana-gateway-https-envoy\n    protocol: HTTPS\n    port: 443\n    allowedRoutes:\n        namespaces:\n          from: All\n    hostname: \"grafana.e4r.internal\"\n    tls:\n      certificateRefs:\n      - kind: Secret\n        group: \"\"\n        name: grafana-gateway-tls\n\n---\n\napiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\n  name: grafana\n  namespace: monitoring\nspec:\n  parentRefs:\n    - name: envoy-grafana-gateway # Updated to use the new monitoring-gateway\n      namespace: default         # Ensure the gateway is in the monitoring namespace\n  hostnames:\n    - \"grafana.e4r.internal\"\n  rules:\n    - matches:\n        - path:\n            type: PathPrefix\n            value: /\n      backendRefs:\n        - name: prometheus-stack-grafana\n          port: 80\n</code></pre> <p>7. Apply the Gateway API Resources and access the Grafana Dashboard  After applying the Gateway API resources, access the Grafana dashboard at <code>https://grafana.e4r.internal/grafana</code>. The default login credentials are:</p> <p>Username: <code>admin</code> Password: <code>prom-operator</code></p>"},{"location":"observability/#grafana-tools","title":"Grafana Tools:","text":"<pre><code>To install the Grafana tools with customized configurations, follow these steps\n</code></pre> <p>1. Add the Grafana Helm Repository:</p> <pre><code>helm repo add grafana https://grafana.github.io/helm-charts\nhelm repo update\n</code></pre> <p>2. Grafana Loki:</p> <p>There are three different ways of deploying the Grafana Loki, they are</p> <ol> <li>Monolithic</li> <li>Simple Scalable</li> <li>Microservice</li> </ol> <pre><code> helm show values grafana/loki &gt; values.yaml\n</code></pre> <p>This command saves the default values to a <code>values.yaml</code> file, which you can edit to tailor the installation to your requirements. Customise the values.yaml according to your requirements and in this case, we are installing a simple monolithic version of loki and the values.yaml looks like this</p> <pre><code>loki:\n  auth_enabled: false\n  commonConfig:\n    replication_factor: 1\n  schemaConfig:\n    configs:\n      - from: \"2025-04-01\"\n        store: tsdb\n        object_store: s3\n        schema: v13\n        index:\n          prefix: loki_index_\n          period: 24h\n  pattern_ingester:\n      enabled: true\n  limits_config:\n    allow_structured_metadata: true\n    volume_enabled: true\n  ruler:\n    enable_api: true\n\nminio:\n  enabled: true\n\ndeploymentMode: SingleBinary\n\nsingleBinary:\n  replicas: 1\n\nbackend:\n  replicas: 0\nread:\n  replicas: 0\nwrite:\n  replicas: 0\ningester:\n  replicas: 0\nquerier:\n  replicas: 0\nqueryFrontend:\n  replicas: 0\nqueryScheduler:\n  replicas: 0\ndistributor:\n  replicas: 0\ncompactor:\n  replicas: 0\nindexGateway:\n  replicas: 0\nbloomCompactor:\n  replicas: 0\nbloomGateway:\n  replicas: 0\n</code></pre> <p>Now, we install the Grafana loki with these values as we are willing to deploy it as simple monolithic application in monitoring namespace.</p> <pre><code>helm install loki grafana/loki -n monitoring -f values.yaml\n</code></pre> <p>Now Select the Data source from Connections in the sidebar in the Grafana dashboard and add loki data source and configure it with loki url which is http://loki.monitoring.svc.cluster.local:3100 In our case, add authentication methods if configured to the Loki and save it.</p> <p></p> <p>3. Grafana Alloy:</p> <p>Installing Alloy using helm is straightforward.</p> <pre><code> helm show values grafana/alloy &gt; values.yaml\n</code></pre> <p>This command saves the default values to a <code>values.yaml</code> file, which you can edit to tailor the installation to your requirements. Customise the values.yaml according to your requirements and in this case</p> <pre><code>alloy:\n  configMap:\n    content: |-\n      loki.write \"default\" {\n        endpoint {\n          url = \"http://loki.monitoring:3100/loki/api/v1/push\"\n        }\n      }\n\n      discovery.kubernetes \"pod\" {\n        role = \"pod\"\n      }\n\n      discovery.relabel \"pod_logs\" {\n        targets = discovery.kubernetes.pod.targets\n        rule {\n          source_labels = [\"__meta_kubernetes_namespace\"]\n          action        = \"replace\"\n          target_label  = \"namespace\"\n        }\n        rule {\n          source_labels = [\"__meta_kubernetes_pod_name\"]\n          action        = \"replace\"\n          target_label  = \"pod\"\n        }\n        rule {\n          source_labels = [\"__meta_kubernetes_pod_container_name\"]\n          action        = \"replace\"\n          target_label  = \"container\"\n        }\n        rule {\n          source_labels = [\"__meta_kubernetes_pod_label_app_kubernetes_io_name\"]\n          action        = \"replace\"\n          target_label  = \"app\"\n        }\n        rule {\n          source_labels = [\"__meta_kubernetes_namespace\", \"__meta_kubernetes_pod_container_name\"]\n          action        = \"replace\"\n          target_label  = \"job\"\n          separator     = \"/\"\n          replacement   = \"$1/$2\"\n        }\n        rule {\n          source_labels = [\"__meta_kubernetes_pod_uid\", \"__meta_kubernetes_pod_container_name\"]\n          action        = \"replace\"\n          target_label  = \"__path__\"\n          separator     = \"/\"\n          replacement   = \"/var/log/pods/*$1/*.log\"\n        }\n        rule {\n          source_labels = [\"__meta_kubernetes_pod_container_id\"]\n          action        = \"replace\"\n          target_label  = \"container_runtime\"\n          regex         = \"^(\\\\S+):\\\\/\\\\/.+$\"\n          replacement   = \"$1\"\n        }\n      }\n\n      loki.source.kubernetes \"pod_logs\" {\n        targets    = discovery.relabel.pod_logs.output\n        forward_to = [loki.process.pod_logs.receiver]\n      }\n\n      loki.process \"pod_logs\" {\n        stage.static_labels {\n          values = {\n            cluster = \"kubernetes\",\n          }\n        }\n        forward_to = [loki.write.default.receiver]\n      }\n</code></pre> <p>Alloy does these things in our case</p> <ol> <li>Define Loki Write Endpoint</li> <li>Enable Kubernetes Pod Discovery</li> <li>Configure Relabeling for Log Metadata</li> <li>Define Log Source from Kubernetes Pods</li> <li>Process Logs Before Writing</li> </ol> <p>In order to easily configure the alloy with various components Grafana Alloy has a open source tool, https://grafana.github.io/alloy-configurator/ [ Alloy Configurator ]</p> <p>Now, we install the Grafana alloy with these values in the monitoring namespace.</p> <pre><code>helm install alloy grafana/alloy -n monitoring -f values.yaml\n</code></pre> <p>Grafana Alloy also provides a UI dashboard which should look like this and helps to verify if it\u2019s configured properly.</p> <p></p> <p>4. Grafana Tempo:</p> <p>Grafana Tempo can be installed using Helm, and tracing information can then be provided to it via Grafana Alloy from the services. Optionally, we can install and set up an OpenTelemetry Collector to capture the instrumentation data, which would then be sent to Grafana Alloy. This approach adds an additional step in the pipeline, allowing us to decouple the tracing process or add more flexibility in how traces are fetched from services to Tempo</p> <p>We need to install Grafana Tempo and update the values of Grafana Alloy to enable the capture of traces from services and write them to Tempo. Afterward, we must add Grafana Tempo as a data source in the Grafana dashboard.</p> <p>Installing Grafana Tempo</p> <pre><code>helm install tempo grafana/tempo -n monitoring -f tempo-values.yaml\n</code></pre> <p>Now update the values.yaml of Grafana Alloy to allow it to collect and send traces to Tempo, which looks like this,</p> <pre><code>alloy:\n  extraPorts:\n    - name: \"grpc\"\n      port: 4317\n      targetPort: 4317\n      protocol: \"TCP\"\n  configMap:\n    content: |-\n      ... previous config\n      otelcol.exporter.otlp \"tempoExporter\" {\n        client {\n          endpoint = \"tempo.monitoring.svc.cluster.local:4317\"\n          tls {\n            insecure = true\n          }\n        }\n      }\n      otelcol.connector.servicegraph \"default\" {\n        dimensions = [\"http.method\", \"http.target\"]\n        output {\n          metrics = [otelcol.exporter.otlp.tempoExporter.input]\n        }\n      }\n      otelcol.receiver.otlp \"defaultReceiver\" {\n        grpc {\n          endpoint = \":4317\"\n        }\n        output {\n          traces = [\n            otelcol.exporter.otlp.tempoExporter.input,\n         otelcol.connector.servicegraph.default.input,\n          ]\n        }\n      }\n</code></pre> <p>This configuration allows alloy to collect and send instrumentation data from services to grafana tempo. Now we have to helm upgrade alloy and we should be able to see the new Alloy dashboard graph like this. </p> <p>Now Select the Data source from Connections in the sidebar in the Grafana dashboard and add Tempo data source and configure it with loki url which is http://tempo.monitoring.svc.cluster.local:3100 In our case, add authentication methods if configured to the Loki and save it. Also add the trace to logs and trace to metrics data sources, which can be Grafana Loki and Prometheus for trace to metrics and save it.</p> <p></p>"},{"location":"observability/#sample-service-with-prometheus-and-opentelemetry-sdk","title":"Sample Service with Prometheus and OpenTelemetry SDK","text":"<p>In the current implementation, logs can be automatically populated with the help of Grafana Alloy. However, for metrics and tracing, we need to install and configure from the respective libraries. To avoid code-level changes, we considered alternatives such as auto-instrumentation for tracing and a sidecar approach, or other alternatives for metrics. For now, we have created two test services in FastAPI (Python) and NodeJs (Javascript) that have Prometheus and OpenTelemetry clients installed. These services will capture metrics and logs and send them to Prometheus and Tempo, respectively.</p> <p>Test App for Metrics</p> <pre><code>const express = require(\"express\");\nconst http = require(\"http\");\nconst promClient = require(\"prom-client\");\n\nconst collectDefaultMetrics = promClient.collectDefaultMetrics;\ncollectDefaultMetrics({ register: promClient.register });\n\nconst apiHits = new promClient.Counter({\n  name: \"api_hits\",\n  help: \"Number of hits to the API\",\n});\nconst httpRequests = new promClient.Counter({\n  name: \"http_requests\",\n  help: \"Number of HTTP requests\",\n  labelNames: [\"route\", \"method\", \"status\"],\n});\n\nconst app = express();\nconst server = http.createServer(app);\nserver.listen(3000, () =&gt; {\n  console.log(\"Server is running on port 3000\");\n});\n\napp.get(\"/fast\", (_, res) =&gt; {\n  apiHits.inc();\n  httpRequests.inc({ route: \"/fast\", method: \"GET\", status: 200 });\n  try {\n    console.log(\"This is a fast request hit!\");\n    res.send(\"This is a fast request!\");\n  } catch (error) {\n    logger.error(\"An error occurred!\");\n    res.status(500).send(\"An error occurred!\");\n  }\n});\n\napp.get(\"/slow\", async (_, res) =&gt; {\n  apiHits.inc();\n  httpRequests.inc({ route: \"/slow\", method: \"GET\", status: 200 });\n  try {\n    console.log(\"This is a slow request hit!\");\n    await new Promise((resolve) =&gt; setTimeout(resolve, 3000));\n    res.send(\"This is a slow request!\");\n  } catch (error) {\n    logger.error(\"An error occurred!\");\n    res.status(500).send(\"An error occurred!\");\n  }\n});\n\napp.get(\"/metrics\", async (_, res) =&gt; {\n  console.log(\"This is a metrics request hit!\");\n  const metrics = await promClient.register.metrics();\n  res.setHeader(\"Content-Type\", promClient.register.contentType);\n  res.send(metrics);\n});\n</code></pre> <p>When deploying this service make sure to add a ServiceMonitor in order to allow Prometheus to scrape the metrics from the service periodically</p> <pre><code>apiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: k8s-app-http\n  namespace: services\n  labels:\n    release: prometheus-stack\nspec:\n  selector:\n    matchLabels:\n      app: test-app\n  endpoints:\n  - port: http-metrics\n    path: /metrics\n    interval: 15s\n</code></pre> <p>Test App for Tracing</p> <pre><code>import time\nimport random\nimport logging\nimport http.client\n\nfrom fastapi import FastAPI\nfrom opentelemetry import trace\nfrom opentelemetry.instrumentation.fastapi import FastAPIInstrumentor\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.resources import Resource\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor, ConsoleSpanExporter\nfrom opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger('opentelemetry')\nlogger.setLevel(logging.INFO)\n\napp = FastAPI()\n\nresource = Resource(attributes={\n    \"service.name\": \"test-api-service\"\n})\n\nprovider = TracerProvider(resource=resource)\n\notel_endpoint_url = \"http://otel-collector-opentelemetry-collector.monitoring.svc.cluster.local:4317\"\nif otel_endpoint_url:\n    otlp_exporter = OTLPSpanExporter(endpoint=otel_endpoint_url)\n    processor = BatchSpanProcessor(otlp_exporter)\nelse:\n    processor = BatchSpanProcessor(ConsoleSpanExporter())\n\nprovider.add_span_processor(processor)\n\ntrace.set_tracer_provider(provider)\ntracer = trace.get_tracer(__name__)\n\nFastAPIInstrumentor.instrument_app(app)\n\nFAST_API_HOST = \"test-app.services.svc.cluster.local\"\nSLOW_API_HOST = \"test-app.services.svc.cluster.local\"\nFAST_API_PORT = 3000\nSLOW_API_PORT = 3000\n\n@app.get(\"/fast\")\nasync def fast_endpoint():\n    with tracer.start_as_current_span(\"fast_operation\"):\n        print(\"Fast request started\")\n        conn = http.client.HTTPConnection(FAST_API_HOST, FAST_API_PORT)\n        conn.request(\"GET\", \"/fast\")\n        print(\"API call to test service started\")\n        response = conn.getresponse()\n        data = response.read().decode('utf-8')\n        conn.close()\n        print(\"API call to test service ended\")\n        print(\"Fast request completed\")\n        return {\"message\": \"Fast response\"}\n\n@app.get(\"/slow\")\nasync def slow_endpoint():\n    with tracer.start_as_current_span(\"slow_operation\"):\n        print(\"Slow request started\")\n        conn = http.client.HTTPConnection(SLOW_API_HOST, SLOW_API_PORT)\n        conn.request(\"GET\", \"/slow\")\n        print(\"API call to test service started\")\n        response = conn.getresponse()\n        data = response.read().decode('utf-8')\n        conn.close()\n        print(\"API call to test service ended\")\n        time.sleep(random.uniform(1, 3))\n        print(\"Slow request completed\")\n        return {\"message\": \"Slow response\"}\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n</code></pre>"},{"location":"observability/#official-docs-and-helm-charts","title":"Official Docs and Helm Charts","text":"<ul> <li>https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack</li> <li>https://github.com/grafana/helm-charts/</li> <li>https://github.com/open-telemetry/opentelemetry-helm-charts/tree/main/charts/opentelemetry-collector</li> <li>https://grafana.com/docs/tempo/latest/getting-started/</li> <li>https://grafana.com/docs/alloy/latest/</li> <li>https://grafana.com/docs/loki/latest/</li> </ul>"},{"location":"terraform/","title":"terrform","text":""},{"location":"terraform/#terraform-scripts","title":"Terraform scripts","text":""},{"location":"terraform/#software-discovery-service","title":"Software discovery service","text":"<pre><code># Navigate to the terraform/software-discovery directory\ncd terraform/software-discovery      \n\n# Preview the changes that Terraform will make using the specified variable file\nterraform plan -var-file=../vars-files/software-discovery.tfvars\n\n# Apply the changes automatically without prompting for approval, using the specified variable file\nterraform apply --auto-approve -var-file=../vars-files/software-discovery.tfvars\n\n# Display the current state of the infrastructure managed by Terraform\nterraform show\n\n# Destroy the infrastructure automatically without prompting for approval, using the specified variable file\nterraform destroy --auto-approve -var-file=../vars-files/software-discovery.tfvars\n</code></pre>"}]}